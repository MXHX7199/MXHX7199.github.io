---
layout: archive
title: "Interests & Projects"
permalink: /selected-projects/
author_profile: true
redirect_from:
  - /Selected-Projects
---
<table width="100%" align="center" border="0" cellpadding="20">
  <tbody>
    <!-- 第二行：文本部分 -->
    <tr>
      <td valign="top">
        <p>
          <strong><font color="#1b6fc2">Efficient LLM</font></strong>
          <br>
          <!-- <strong>SJTU</strong>, ACA Lab <a href="">XXX</a>, Sept. 2023 - July 2024 <br> -->
        </p>
        <p><br>
          LLM models are over 1000× larger than traditional AI, creating major efficiency challenges. We focus on enabling efficient LLM acceleration through compression-based (such as quantization, sparsity, etc.) software-hardware co-design across ASICs, FPGAs, GPUs, and PIMs. Our methods support compressed inference, fine-tuning, and training, aligning model structures with underlying hardware for optimal performance. We further explore KV cache compression, sparse attention, and long-context learning to reduce runtime cost. Beyond traditional platforms, we explore neuromorphic computing paradigms, leveraging event-driven SNNs and in-memory computing to realize orders-of-magnitude gains in energy efficiency.
        </p>
      </td>
    </tr>
    <!-- 第一行：图片部分 -->
    <tr>
      <td align="center">
        <div style="display: flex; flex-direction: column; width: 100%; height: 100vh;">
          <iframe class="rounded shadow media-img" src="../images/EfficientLLM.html" style="width: 100%; height: 100%;"></iframe>
        </div>
      </td>
    </tr>
  </tbody>
</table>


